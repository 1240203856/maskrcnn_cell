1、未加入attention之前 、改变backbone做对比实验，找出效果最好的3个backbone
2、在选出效果最好的3个backbone之上加入attention，做对比实验  （不能再使用预训练模型）
3、最终对比：普通maskrcnn、改变backbone之后的maskrcnn、加入attention之后的maskrcnn  做对比
             
batch_size：4
epochs：200
learning_rate：00005

学习率改变方式：
torch.optim.lr_scheduler.StepLR(optimizer,step_size=20,gamma=0.1)

训练集由1000*1000的大图切分为200*200的小图，训练时进行了随机翻转，训练与测试时都进行了归一化处理
训练集：16*25
测试集：14*25（same_tissue:8 / diff_tissue:6）

list to do：
对另外两个模型加入attention
